{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run from within cloned CLEAN repo so that it's easier\n",
    "1. Clone the CLEAN repo\n",
    "2. Follow the installation instruction\n",
    "3. Move this file to `CLEAN/app` (should be in same folder as demo.ipynb)\n",
    "4. Move `EC2protein_train.csv` and `price_protein_test.csv` to `CLEAN/app/data`\n",
    "5. Run this notebook from the CLEAN directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLEAN.utils import *\n",
    "import pandas as pd\n",
    "ensure_dirs(\"data/esm_data\")\n",
    "ensure_dirs(\"data/pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to the correct format for clean\n",
    "test_set_list = ['30_protein_test', '30-50_protein_test', 'price_protein_test', 'promiscuous_protein_test']\n",
    "for test_set in test_set_list:\n",
    "    df = pd.read_csv('./data/{}.csv')\n",
    "    df = df[['Entry', 'EC number', 'Sequence']]\n",
    "    df.to_csv('./data/{}.csv'.format(test_set), sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training\n",
    "might be better to do this in a script if it takes a long time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CLEAN.utils import mutate_single_seq_ECs, retrive_esm1b_embedding, compute_esm_distance\n",
    "train_set = \"protein_train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Entry'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(train_set))\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEntry\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSequence\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEC number\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlist\u001b[39m(x)})\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[1;32m      3\u001b[0m df\n",
      "File \u001b[0;32m~/miniconda3/envs/clean/lib/python3.10/site-packages/pandas/core/frame.py:7712\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, observed, dropna)\u001b[0m\n\u001b[1;32m   7707\u001b[0m axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n\u001b[1;32m   7709\u001b[0m \u001b[38;5;66;03m# https://github.com/python/mypy/issues/7642\u001b[39;00m\n\u001b[1;32m   7710\u001b[0m \u001b[38;5;66;03m# error: Argument \"squeeze\" to \"DataFrameGroupBy\" has incompatible type\u001b[39;00m\n\u001b[1;32m   7711\u001b[0m \u001b[38;5;66;03m# \"Union[bool, NoDefault]\"; expected \"bool\"\u001b[39;00m\n\u001b[0;32m-> 7712\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameGroupBy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7715\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mas_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7718\u001b[0m \u001b[43m    \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroup_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7720\u001b[0m \u001b[43m    \u001b[49m\u001b[43msqueeze\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   7721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7723\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/clean/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:882\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, observed, mutated, dropna)\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgroupby\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgrouper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_grouper\n\u001b[0;32m--> 882\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m \u001b[43mget_grouper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobserved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobserved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmutated\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmutated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdropna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj \u001b[38;5;241m=\u001b[39m obj\n\u001b[1;32m    894\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxis \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_get_axis_number(axis)\n",
      "File \u001b[0;32m~/miniconda3/envs/clean/lib/python3.10/site-packages/pandas/core/groupby/grouper.py:882\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, mutated, validate, dropna)\u001b[0m\n\u001b[1;32m    880\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m    883\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    884\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m    885\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Entry'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('./data/{}.csv'.format(train_set))\n",
    "df = df.groupby(['Entry', 'Sequence']).agg({'EC number': lambda x: list(x)}).reset_index()\n",
    "df['EC number'] = df['EC number'].apply(lambda x: ';'.join(x))\n",
    "df = df[['Entry', 'EC number', 'Sequence']]\n",
    "df.to_csv('./data/{}.csv'.format(train_set), sep='\\t', index=False)\n",
    "csv_to_fasta(\"data/{}.csv\".format(train_set), \"data/{}.fasta\".format(train_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps below are a bit slower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n",
      "Read data/protein2EC_train.fasta with 1000 sequences\n",
      "Processing 1 of 73 batches (37 sequences)\n",
      "Processing 2 of 73 batches (33 sequences)\n",
      "Processing 3 of 73 batches (32 sequences)\n",
      "Processing 4 of 73 batches (31 sequences)\n",
      "Processing 5 of 73 batches (31 sequences)\n",
      "Processing 6 of 73 batches (29 sequences)\n",
      "Processing 7 of 73 batches (26 sequences)\n",
      "Processing 8 of 73 batches (25 sequences)\n",
      "Processing 9 of 73 batches (23 sequences)\n",
      "Processing 10 of 73 batches (21 sequences)\n",
      "Processing 11 of 73 batches (21 sequences)\n",
      "Processing 12 of 73 batches (20 sequences)\n",
      "Processing 13 of 73 batches (19 sequences)\n",
      "Processing 14 of 73 batches (18 sequences)\n",
      "Processing 15 of 73 batches (18 sequences)\n",
      "Processing 16 of 73 batches (17 sequences)\n",
      "Processing 17 of 73 batches (17 sequences)\n",
      "Processing 18 of 73 batches (17 sequences)\n",
      "Processing 19 of 73 batches (16 sequences)\n",
      "Processing 20 of 73 batches (16 sequences)\n",
      "Processing 21 of 73 batches (15 sequences)\n",
      "Processing 22 of 73 batches (15 sequences)\n",
      "Processing 23 of 73 batches (14 sequences)\n",
      "Processing 24 of 73 batches (14 sequences)\n",
      "Processing 25 of 73 batches (14 sequences)\n",
      "Processing 26 of 73 batches (13 sequences)\n",
      "Processing 27 of 73 batches (13 sequences)\n",
      "Processing 28 of 73 batches (13 sequences)\n",
      "Processing 29 of 73 batches (13 sequences)\n",
      "Processing 30 of 73 batches (13 sequences)\n",
      "Processing 31 of 73 batches (12 sequences)\n",
      "Processing 32 of 73 batches (12 sequences)\n",
      "Processing 33 of 73 batches (12 sequences)\n",
      "Processing 34 of 73 batches (12 sequences)\n",
      "Processing 35 of 73 batches (11 sequences)\n",
      "Processing 36 of 73 batches (11 sequences)\n",
      "Processing 37 of 73 batches (11 sequences)\n",
      "Processing 38 of 73 batches (11 sequences)\n",
      "Processing 39 of 73 batches (11 sequences)\n",
      "Processing 40 of 73 batches (11 sequences)\n",
      "Processing 41 of 73 batches (10 sequences)\n",
      "Processing 42 of 73 batches (10 sequences)\n",
      "Processing 43 of 73 batches (10 sequences)\n",
      "Processing 44 of 73 batches (10 sequences)\n",
      "Processing 45 of 73 batches (10 sequences)\n",
      "Processing 46 of 73 batches (10 sequences)\n",
      "Processing 47 of 73 batches (9 sequences)\n",
      "Processing 48 of 73 batches (9 sequences)\n",
      "Processing 49 of 73 batches (9 sequences)\n",
      "Processing 50 of 73 batches (9 sequences)\n",
      "Processing 51 of 73 batches (9 sequences)\n",
      "Processing 52 of 73 batches (9 sequences)\n",
      "Processing 53 of 73 batches (9 sequences)\n",
      "Processing 54 of 73 batches (9 sequences)\n",
      "Processing 55 of 73 batches (9 sequences)\n",
      "Processing 56 of 73 batches (9 sequences)\n",
      "Processing 57 of 73 batches (9 sequences)\n",
      "Processing 58 of 73 batches (9 sequences)\n",
      "Processing 59 of 73 batches (8 sequences)\n",
      "Processing 60 of 73 batches (8 sequences)\n",
      "Processing 61 of 73 batches (8 sequences)\n",
      "Processing 62 of 73 batches (8 sequences)\n",
      "Processing 63 of 73 batches (8 sequences)\n",
      "Processing 64 of 73 batches (8 sequences)\n",
      "Processing 65 of 73 batches (8 sequences)\n",
      "Processing 66 of 73 batches (8 sequences)\n",
      "Processing 67 of 73 batches (8 sequences)\n",
      "Processing 68 of 73 batches (8 sequences)\n",
      "Processing 69 of 73 batches (8 sequences)\n",
      "Processing 70 of 73 batches (8 sequences)\n",
      "Processing 71 of 73 batches (8 sequences)\n",
      "Processing 72 of 73 batches (8 sequences)\n",
      "Processing 73 of 73 batches (2 sequences)\n"
     ]
    }
   ],
   "source": [
    "retrive_esm1b_embedding(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of EC numbers with only one sequences: 219\n",
      "Number of single-seq EC number sequences need to mutate:  204\n",
      "Number of single-seq EC numbers already mutated:  15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n",
      "Read data/protein2EC_train_single_seq_ECs.fasta with 2040 sequences\n",
      "Processing 1 of 237 batches (25 sequences)\n",
      "Processing 2 of 237 batches (24 sequences)\n",
      "Processing 3 of 237 batches (23 sequences)\n",
      "Processing 4 of 237 batches (22 sequences)\n",
      "Processing 5 of 237 batches (21 sequences)\n",
      "Processing 6 of 237 batches (20 sequences)\n",
      "Processing 7 of 237 batches (19 sequences)\n",
      "Processing 8 of 237 batches (18 sequences)\n",
      "Processing 9 of 237 batches (18 sequences)\n",
      "Processing 10 of 237 batches (17 sequences)\n",
      "Processing 11 of 237 batches (17 sequences)\n",
      "Processing 12 of 237 batches (17 sequences)\n",
      "Processing 13 of 237 batches (16 sequences)\n",
      "Processing 14 of 237 batches (16 sequences)\n",
      "Processing 15 of 237 batches (16 sequences)\n",
      "Processing 16 of 237 batches (16 sequences)\n",
      "Processing 17 of 237 batches (15 sequences)\n",
      "Processing 18 of 237 batches (15 sequences)\n",
      "Processing 19 of 237 batches (15 sequences)\n",
      "Processing 20 of 237 batches (15 sequences)\n",
      "Processing 21 of 237 batches (15 sequences)\n",
      "Processing 22 of 237 batches (14 sequences)\n",
      "Processing 23 of 237 batches (14 sequences)\n",
      "Processing 24 of 237 batches (14 sequences)\n",
      "Processing 25 of 237 batches (14 sequences)\n",
      "Processing 26 of 237 batches (14 sequences)\n",
      "Processing 27 of 237 batches (13 sequences)\n",
      "Processing 28 of 237 batches (13 sequences)\n",
      "Processing 29 of 237 batches (13 sequences)\n",
      "Processing 30 of 237 batches (13 sequences)\n",
      "Processing 31 of 237 batches (13 sequences)\n",
      "Processing 32 of 237 batches (13 sequences)\n",
      "Processing 33 of 237 batches (13 sequences)\n",
      "Processing 34 of 237 batches (12 sequences)\n",
      "Processing 35 of 237 batches (12 sequences)\n",
      "Processing 36 of 237 batches (12 sequences)\n",
      "Processing 37 of 237 batches (12 sequences)\n",
      "Processing 38 of 237 batches (12 sequences)\n",
      "Processing 39 of 237 batches (12 sequences)\n",
      "Processing 40 of 237 batches (12 sequences)\n",
      "Processing 41 of 237 batches (12 sequences)\n",
      "Processing 42 of 237 batches (12 sequences)\n",
      "Processing 43 of 237 batches (12 sequences)\n",
      "Processing 44 of 237 batches (12 sequences)\n",
      "Processing 45 of 237 batches (12 sequences)\n",
      "Processing 46 of 237 batches (11 sequences)\n",
      "Processing 47 of 237 batches (11 sequences)\n",
      "Processing 48 of 237 batches (11 sequences)\n",
      "Processing 49 of 237 batches (11 sequences)\n",
      "Processing 50 of 237 batches (11 sequences)\n",
      "Processing 51 of 237 batches (11 sequences)\n",
      "Processing 52 of 237 batches (11 sequences)\n",
      "Processing 53 of 237 batches (11 sequences)\n",
      "Processing 54 of 237 batches (11 sequences)\n",
      "Processing 55 of 237 batches (11 sequences)\n",
      "Processing 56 of 237 batches (11 sequences)\n",
      "Processing 57 of 237 batches (11 sequences)\n",
      "Processing 58 of 237 batches (11 sequences)\n",
      "Processing 59 of 237 batches (10 sequences)\n",
      "Processing 60 of 237 batches (10 sequences)\n",
      "Processing 61 of 237 batches (10 sequences)\n",
      "Processing 62 of 237 batches (10 sequences)\n",
      "Processing 63 of 237 batches (10 sequences)\n",
      "Processing 64 of 237 batches (10 sequences)\n",
      "Processing 65 of 237 batches (10 sequences)\n",
      "Processing 66 of 237 batches (10 sequences)\n",
      "Processing 67 of 237 batches (10 sequences)\n",
      "Processing 68 of 237 batches (10 sequences)\n",
      "Processing 69 of 237 batches (10 sequences)\n",
      "Processing 70 of 237 batches (10 sequences)\n",
      "Processing 71 of 237 batches (10 sequences)\n",
      "Processing 72 of 237 batches (10 sequences)\n",
      "Processing 73 of 237 batches (10 sequences)\n",
      "Processing 74 of 237 batches (9 sequences)\n",
      "Processing 75 of 237 batches (9 sequences)\n",
      "Processing 76 of 237 batches (9 sequences)\n",
      "Processing 77 of 237 batches (9 sequences)\n",
      "Processing 78 of 237 batches (9 sequences)\n",
      "Processing 79 of 237 batches (9 sequences)\n",
      "Processing 80 of 237 batches (9 sequences)\n",
      "Processing 81 of 237 batches (9 sequences)\n",
      "Processing 82 of 237 batches (9 sequences)\n",
      "Processing 83 of 237 batches (9 sequences)\n",
      "Processing 84 of 237 batches (9 sequences)\n",
      "Processing 85 of 237 batches (9 sequences)\n",
      "Processing 86 of 237 batches (9 sequences)\n",
      "Processing 87 of 237 batches (9 sequences)\n",
      "Processing 88 of 237 batches (9 sequences)\n",
      "Processing 89 of 237 batches (9 sequences)\n",
      "Processing 90 of 237 batches (9 sequences)\n",
      "Processing 91 of 237 batches (9 sequences)\n",
      "Processing 92 of 237 batches (9 sequences)\n",
      "Processing 93 of 237 batches (8 sequences)\n",
      "Processing 94 of 237 batches (8 sequences)\n",
      "Processing 95 of 237 batches (8 sequences)\n",
      "Processing 96 of 237 batches (8 sequences)\n",
      "Processing 97 of 237 batches (8 sequences)\n",
      "Processing 98 of 237 batches (8 sequences)\n",
      "Processing 99 of 237 batches (8 sequences)\n",
      "Processing 100 of 237 batches (8 sequences)\n",
      "Processing 101 of 237 batches (8 sequences)\n",
      "Processing 102 of 237 batches (8 sequences)\n",
      "Processing 103 of 237 batches (8 sequences)\n",
      "Processing 104 of 237 batches (8 sequences)\n",
      "Processing 105 of 237 batches (8 sequences)\n",
      "Processing 106 of 237 batches (8 sequences)\n",
      "Processing 107 of 237 batches (8 sequences)\n",
      "Processing 108 of 237 batches (8 sequences)\n",
      "Processing 109 of 237 batches (8 sequences)\n",
      "Processing 110 of 237 batches (8 sequences)\n",
      "Processing 111 of 237 batches (8 sequences)\n",
      "Processing 112 of 237 batches (8 sequences)\n",
      "Processing 113 of 237 batches (8 sequences)\n",
      "Processing 114 of 237 batches (8 sequences)\n",
      "Processing 115 of 237 batches (8 sequences)\n",
      "Processing 116 of 237 batches (8 sequences)\n",
      "Processing 117 of 237 batches (8 sequences)\n",
      "Processing 118 of 237 batches (7 sequences)\n",
      "Processing 119 of 237 batches (7 sequences)\n",
      "Processing 120 of 237 batches (7 sequences)\n",
      "Processing 121 of 237 batches (7 sequences)\n",
      "Processing 122 of 237 batches (7 sequences)\n",
      "Processing 123 of 237 batches (7 sequences)\n",
      "Processing 124 of 237 batches (7 sequences)\n",
      "Processing 125 of 237 batches (7 sequences)\n",
      "Processing 126 of 237 batches (7 sequences)\n",
      "Processing 127 of 237 batches (7 sequences)\n",
      "Processing 128 of 237 batches (7 sequences)\n",
      "Processing 129 of 237 batches (7 sequences)\n",
      "Processing 130 of 237 batches (7 sequences)\n",
      "Processing 131 of 237 batches (7 sequences)\n",
      "Processing 132 of 237 batches (7 sequences)\n",
      "Processing 133 of 237 batches (7 sequences)\n",
      "Processing 134 of 237 batches (7 sequences)\n",
      "Processing 135 of 237 batches (7 sequences)\n",
      "Processing 136 of 237 batches (7 sequences)\n",
      "Processing 137 of 237 batches (7 sequences)\n",
      "Processing 138 of 237 batches (7 sequences)\n",
      "Processing 139 of 237 batches (7 sequences)\n",
      "Processing 140 of 237 batches (7 sequences)\n",
      "Processing 141 of 237 batches (6 sequences)\n",
      "Processing 142 of 237 batches (6 sequences)\n",
      "Processing 143 of 237 batches (6 sequences)\n",
      "Processing 144 of 237 batches (6 sequences)\n",
      "Processing 145 of 237 batches (6 sequences)\n",
      "Processing 146 of 237 batches (6 sequences)\n",
      "Processing 147 of 237 batches (6 sequences)\n",
      "Processing 148 of 237 batches (6 sequences)\n",
      "Processing 149 of 237 batches (6 sequences)\n",
      "Processing 150 of 237 batches (6 sequences)\n",
      "Processing 151 of 237 batches (6 sequences)\n",
      "Processing 152 of 237 batches (6 sequences)\n",
      "Processing 153 of 237 batches (6 sequences)\n",
      "Processing 154 of 237 batches (6 sequences)\n",
      "Processing 155 of 237 batches (6 sequences)\n",
      "Processing 156 of 237 batches (6 sequences)\n",
      "Processing 157 of 237 batches (6 sequences)\n",
      "Processing 158 of 237 batches (6 sequences)\n",
      "Processing 159 of 237 batches (6 sequences)\n",
      "Processing 160 of 237 batches (6 sequences)\n",
      "Processing 161 of 237 batches (6 sequences)\n",
      "Processing 162 of 237 batches (6 sequences)\n",
      "Processing 163 of 237 batches (6 sequences)\n",
      "Processing 164 of 237 batches (6 sequences)\n",
      "Processing 165 of 237 batches (6 sequences)\n",
      "Processing 166 of 237 batches (6 sequences)\n",
      "Processing 167 of 237 batches (6 sequences)\n",
      "Processing 168 of 237 batches (6 sequences)\n",
      "Processing 169 of 237 batches (6 sequences)\n",
      "Processing 170 of 237 batches (6 sequences)\n",
      "Processing 171 of 237 batches (6 sequences)\n",
      "Processing 172 of 237 batches (6 sequences)\n",
      "Processing 173 of 237 batches (6 sequences)\n",
      "Processing 174 of 237 batches (6 sequences)\n",
      "Processing 175 of 237 batches (6 sequences)\n",
      "Processing 176 of 237 batches (6 sequences)\n",
      "Processing 177 of 237 batches (6 sequences)\n",
      "Processing 178 of 237 batches (6 sequences)\n",
      "Processing 179 of 237 batches (6 sequences)\n",
      "Processing 180 of 237 batches (6 sequences)\n",
      "Processing 181 of 237 batches (6 sequences)\n",
      "Processing 182 of 237 batches (6 sequences)\n",
      "Processing 183 of 237 batches (6 sequences)\n",
      "Processing 184 of 237 batches (6 sequences)\n",
      "Processing 185 of 237 batches (6 sequences)\n",
      "Processing 186 of 237 batches (6 sequences)\n",
      "Processing 187 of 237 batches (6 sequences)\n",
      "Processing 188 of 237 batches (6 sequences)\n",
      "Processing 189 of 237 batches (6 sequences)\n",
      "Processing 190 of 237 batches (6 sequences)\n",
      "Processing 191 of 237 batches (5 sequences)\n",
      "Processing 192 of 237 batches (5 sequences)\n",
      "Processing 193 of 237 batches (5 sequences)\n",
      "Processing 194 of 237 batches (5 sequences)\n",
      "Processing 195 of 237 batches (5 sequences)\n",
      "Processing 196 of 237 batches (5 sequences)\n",
      "Processing 197 of 237 batches (5 sequences)\n",
      "Processing 198 of 237 batches (5 sequences)\n",
      "Processing 199 of 237 batches (5 sequences)\n",
      "Processing 200 of 237 batches (5 sequences)\n",
      "Processing 201 of 237 batches (5 sequences)\n",
      "Processing 202 of 237 batches (5 sequences)\n",
      "Processing 203 of 237 batches (5 sequences)\n",
      "Processing 204 of 237 batches (5 sequences)\n",
      "Processing 205 of 237 batches (5 sequences)\n",
      "Processing 206 of 237 batches (5 sequences)\n",
      "Processing 207 of 237 batches (5 sequences)\n",
      "Processing 208 of 237 batches (5 sequences)\n",
      "Processing 209 of 237 batches (5 sequences)\n",
      "Processing 210 of 237 batches (5 sequences)\n",
      "Processing 211 of 237 batches (5 sequences)\n",
      "Processing 212 of 237 batches (5 sequences)\n",
      "Processing 213 of 237 batches (5 sequences)\n",
      "Processing 214 of 237 batches (5 sequences)\n",
      "Processing 215 of 237 batches (5 sequences)\n",
      "Processing 216 of 237 batches (5 sequences)\n",
      "Processing 217 of 237 batches (5 sequences)\n",
      "Processing 218 of 237 batches (5 sequences)\n",
      "Processing 219 of 237 batches (5 sequences)\n",
      "Processing 220 of 237 batches (5 sequences)\n",
      "Processing 221 of 237 batches (5 sequences)\n",
      "Processing 222 of 237 batches (5 sequences)\n",
      "Processing 223 of 237 batches (5 sequences)\n",
      "Processing 224 of 237 batches (5 sequences)\n",
      "Processing 225 of 237 batches (5 sequences)\n",
      "Processing 226 of 237 batches (5 sequences)\n",
      "Processing 227 of 237 batches (5 sequences)\n",
      "Processing 228 of 237 batches (5 sequences)\n",
      "Processing 229 of 237 batches (5 sequences)\n",
      "Processing 230 of 237 batches (5 sequences)\n",
      "Processing 231 of 237 batches (5 sequences)\n",
      "Processing 232 of 237 batches (5 sequences)\n",
      "Processing 233 of 237 batches (5 sequences)\n",
      "Processing 234 of 237 batches (5 sequences)\n",
      "Processing 235 of 237 batches (5 sequences)\n",
      "Processing 236 of 237 batches (4 sequences)\n",
      "Processing 237 of 237 batches (1 sequences)\n"
     ]
    }
   ],
   "source": [
    "#this is to generate masked sequences for examples iwth only one positive\n",
    "masked_fasta_file = mutate_single_seq_ECs(train_set) #get rid of duplicates\n",
    "\n",
    "#retrieving embeddings and distance matrix needs to be done once per training dataset\n",
    "retrive_esm1b_embedding(masked_fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368/368 [00:00<00:00, 34855.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating distance map, number of unique EC is 368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "368it [00:00, 8477.58it/s]\n"
     ]
    }
   ],
   "source": [
    "compute_esm_distance(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #if this step is too long\n",
    "# #make a temporary empty file\n",
    "# model_name = '{}_triplet_2.pth'.format(train_set)\n",
    "# with open('./data/model/{}'.format(model_name), 'w') as fp:\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train with triplet loss for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> device used: cuda:0 | dtype used:  torch.float32 \n",
      "==> args: Namespace(learning_rate=0.0005, epoch=2, model_name='protein2EC_train_triplet', training_data='protein2EC_train', hidden_dim=512, out_dim=128, adaptive_rate=100, verbose=False)\n",
      "The number of unique EC numbers:  368\n",
      "---------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  0.41s | training loss 0.8793\n",
      "---------------------------------------------------------------------------\n",
      "Best from epoch :   2; loss: 0.8413\n",
      "---------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  0.12s | training loss 0.8413\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "!python ./train-triplet.py --training_data protein_train --model_name protein_train_triplet --epoch 7000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> device used: cuda:0 | dtype used:  torch.float32 \n",
      "==> args: Namespace(learning_rate=0.0005, epoch=2, model_name='protein2EC_train_supconH', training_data='protein2EC_train', temp=0.1, n_pos=9, n_neg=30, hidden_dim=512, out_dim=256, adaptive_rate=60, verbose=False)\n",
      "The number of unique EC numbers:  368\n",
      "---------------------------------------------------------------------------\n",
      "| end of epoch   1 | time:  2.32s | training loss 3.5778\n",
      "---------------------------------------------------------------------------\n",
      "Best from epoch :   2; loss: 3.3484\n",
      "---------------------------------------------------------------------------\n",
      "| end of epoch   2 | time:  2.56s | training loss 3.3484\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#this might work better, but it's slower and you need to manually change the dimension size during inference\n",
    "# !python ./train-supconH.py --training_data protein2EC_train --model_name protein2EC_train_supconH --epoch 5250 --n_pos 9 --n_neg 30 -T 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### inference on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transferred model to GPU\n",
      "Read data/price_protein_test.fasta with 148 sequences\n",
      "Processing 1 of 16 batches (15 sequences)\n",
      "Processing 2 of 16 batches (13 sequences)\n",
      "Processing 3 of 16 batches (13 sequences)\n",
      "Processing 4 of 16 batches (11 sequences)\n",
      "Processing 5 of 16 batches (11 sequences)\n",
      "Processing 6 of 16 batches (10 sequences)\n",
      "Processing 7 of 16 batches (10 sequences)\n",
      "Processing 8 of 16 batches (9 sequences)\n",
      "Processing 9 of 16 batches (9 sequences)\n",
      "Processing 10 of 16 batches (8 sequences)\n",
      "Processing 11 of 16 batches (8 sequences)\n",
      "Processing 12 of 16 batches (8 sequences)\n",
      "Processing 13 of 16 batches (7 sequences)\n",
      "Processing 14 of 16 batches (6 sequences)\n",
      "Processing 15 of 16 batches (6 sequences)\n",
      "Processing 16 of 16 batches (4 sequences)\n"
     ]
    }
   ],
   "source": [
    "for test_set in test_set_list:\n",
    "    test_set = 'price_protein_test'\n",
    "    csv_to_fasta(\"data/{}.csv\".format(test_set), \"data/{}.fasta\".format(test_set))\n",
    "    retrive_esm1b_embedding(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add this chunk of code to change the output of CLEAN so that is compatible with our evaluation framework\n",
    "#CLEAN/app/src/CLEAN/infer.py after line 51\n",
    "\n",
    "### manually added this part to save the results in a different format ###\n",
    "# new_df = pd.read_csv('./data/{}.csv'.format(test_data), delimiter='\\t')\n",
    "# num_cols = len(new_df.columns)\n",
    "# #new_df['Entry'] = eval_df.columns\n",
    "# for j in range(len(eval_df)):\n",
    "#     new_df[j] = np.nan\n",
    "\n",
    "# for i, col in enumerate(eval_df.columns):\n",
    "#     sorted_ECs = eval_df[col].sort_values(ascending=True).index.values\n",
    "#     new_df.iloc[i, num_cols:] = sorted_ECs\n",
    "\n",
    "# new_df.to_csv('./results/{}/{}_results_df.csv'.format(train_data, test_data), index=False)\n",
    "### end of manual addition ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### move the trained model to the pretrained folder and rename to the [train_set].pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding sizes for train and test: torch.Size([1060, 128]) torch.Size([148, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368/368 [00:00<00:00, 36703.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating eval distance map, between 148 test ids and 368 train EC cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:00, 7698.64it/s]\n",
      "100%|██████████| 368/368 [00:00<00:00, 34840.50it/s]\n",
      "20000it [00:01, 10726.82it/s]\n",
      "100%|██████████| 148/148 [00:00<00:00, 929.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ EC calling results using random chosen 20k samples ############\n",
      "---------------------------------------------------------------------------\n",
      ">>> total samples: 148 | total ec: 56 \n",
      ">>> precision: 0.0326 | recall: 0.053| F1: 0.0404 | AUC: 0.526 | accuracy: 0.0541 \n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from CLEAN.infer import infer_pvalue\n",
    "\n",
    "for test_set in test_set_list:\n",
    "    infer_pvalue(train_set, test_set, p_value=1e-5, nk_random=20, report_metrics=True, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The embedding sizes for train and test: torch.Size([1060, 128]) torch.Size([148, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 368/368 [00:00<00:00, 49129.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating eval distance map, between 148 test ids and 368 train EC cluster centers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "148it [00:00, 11704.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############ EC calling results using maximum separation ############\n",
      "---------------------------------------------------------------------------\n",
      ">>> total samples: 148 | total ec: 56 \n",
      ">>> precision: 0.0283 | recall: 0.053| F1: 0.0369 | AUC: 0.526 | accuracy: 0.0541 \n",
      "---------------------------------------------------------------------------\n",
      "############ EC calling results using maximum separation ############\n",
      "---------------------------------------------------------------------------\n",
      ">>> total samples: 148 | total ec: 56 \n",
      ">>> precision: 0.0283 | recall: 0.053| F1: 0.0369 | AUC: 0.526 \n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#don't use this for now\n",
    "# from CLEAN.infer import infer_maxsep\n",
    "# infer_maxsep(train_set, test_set, report_metrics=True, pretrained=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
